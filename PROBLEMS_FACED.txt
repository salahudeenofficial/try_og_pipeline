# Issues & Fixes

## 1. Aspect Ratio Issue
Problem: Output was 1664x928 regardless of input aspect ratio
Cause: LightX2V's set_input_info() doesn't pass aspect_ratio for i2i tasks
Fix: Monkey-patch run_pipeline to inject aspect_ratio into input_info + set _auto_resize=False

## 2. Docker Container Compatibility
Problem: Pre-installed flash_attn/sageattention compiled for wrong PyTorch version
Cause: Base images have pre-compiled attention libs that conflict with PyTorch 2.6.0
Fix: Use lightx2v/lightx2v:25101501-cu124 (CUDA 12.4) instead of cu128 version

## 3. LightX2V target_shape Breaking Change (2026-01-12)
Problem: AttributeError: 'LightX2VPipeline' object has no attribute 'target_shape'
Cause: LightX2V commit 0e29a79 ("change target shape input info #749") introduced a breaking change
       that requires the pipeline to have a target_shape attribute for i2i tasks.
       The latest LightX2V main branch is incompatible with our code.
Fix: Revert LightX2V to commit 7651b0f (before the breaking change):
     cd LightX2V && git checkout 7651b0f
     Note: setup_lightx2v.sh has been updated to pin to this commit.

## 4. JSON Serialization Error on Multiple Inference Calls (2026-01-12)
Problem: Object of type QwenImageRunner is not JSON serializable
Cause: Calling pipe.create_generator() multiple times (once per inference) causes 
       LightX2V to try to serialize the runner object, which fails.
Fix: Create the generator ONCE during server startup (in load_models), not per-inference.
     The monkey-patch in run_inference handles aspect ratio dynamically for each image.
