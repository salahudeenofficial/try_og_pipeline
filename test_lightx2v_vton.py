#!/usr/bin/env python3
"""
LightX2V Virtual Try-On Test Script

This script uses the LightX2V framework for high-performance inference
with Qwen-Image-Edit-2511 model for virtual try-on tasks.

Features:
- FP8 quantization support (~50% memory reduction)
- 4-step Lightning LoRA (~10x speedup)
- CPU offloading for low-VRAM GPUs
- CFG parallelism for faster inference

Performance (on L40/L40S with optimizations):
- BF16 + 4-step LoRA: ~6-10 seconds
- FP8 + 4-step distillation: ~5-7 seconds
- FP8 + TeaCache: ~4-6 seconds

L40/L40S Notes:
- Uses Flash Attention 2 (FA3 is Hopper-only)
- Native FP8 support (SM 8.9)
- 48GB VRAM - no offloading needed
"""

import argparse
import gc
import os
import sys
import time
from pathlib import Path

import torch
from PIL import Image

# Set CUDA arch for L40/L40S (Ada Lovelace SM 8.9)
os.environ.setdefault('TORCH_CUDA_ARCH_LIST', '8.9')

# Enable TF32 for faster matmul on Ada Lovelace GPUs
if torch.cuda.is_available():
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False


# Virtual Try-On Prompts
VTON_PROMPT_CN = """Â∞ÜÂõæÁâá 1 ‰∏≠ÁöÑÁªøËâ≤ÈÅÆÁΩ©Âå∫Âüü‰ªÖÁî®‰∫éÂà§Êñ≠ÊúçË£ÖÂ±û‰∫é‰∏äÂçäË∫´Êàñ‰∏ãÂçäË∫´Ôºå‰∏çË¶ÅÂ∞ÜÊúçË£ÖÈôêÂà∂Âú®ÈÅÆÁΩ©ËåÉÂõ¥ÂÜÖ„ÄÇ

Â∞ÜÂõæÁâá 2 ‰∏≠ÁöÑÊúçË£ÖËá™ÁÑ∂Âú∞Á©øÊà¥Âà∞ÂõæÁâá 1 ‰∏≠ÁöÑ‰∫∫Áâ©Ë∫´‰∏äÔºå‰øùÊåÅÂõæÁâá 2 ‰∏≠ÊúçË£ÖÁöÑÂÆåÊï¥ÂΩ¢Áä∂„ÄÅË¢ñÈïøÂíåËΩÆÂªì„ÄÇÊó†ËÆ∫ÂõæÁâá 2 ÊòØÂçïÁã¨ÁöÑÊúçË£ÖÂõæËøòÊòØ‰∫∫Áâ©Á©øÁùÄËØ•ÊúçË£ÖÁöÑÂõæÔºåÈÉΩÂ∫îÂáÜÁ°ÆÂú∞ËΩ¨ÁßªÊúçË£ÖÔºåÂêåÊó∂‰øùÁïôÂÖ∂ÂéüÂßãÈù¢ÊñôË¥®ÊÑü„ÄÅÊùêË¥®ÁªÜËäÇÂíåÈ¢úËâ≤ÂáÜÁ°ÆÊÄß„ÄÇ

Á°Æ‰øùÂõæÁâá 1 ‰∏≠‰∫∫Áâ©ÁöÑÈù¢ÈÉ®„ÄÅÂ§¥ÂèëÂíåÁöÆËÇ§ÂÆåÂÖ®‰øùÊåÅ‰∏çÂèò„ÄÇÂÖâÁÖß‰∏éÈò¥ÂΩ±Â∫îËá™ÁÑ∂ÂåπÈÖçÂõæÁâá 1 ÁöÑÁéØÂ¢ÉÔºå‰ΩÜÊúçË£ÖÁöÑÊùêË¥®Â§ñËßÇÂøÖÈ°ªÂø†ÂÆû‰∫éÂõæÁâá 2„ÄÇ

‰øùÊåÅËæπÁºòÂπ≥ÊªëËûçÂêà„ÄÅÈò¥ÂΩ±ÈÄºÁúüÔºåÊï¥‰ΩìÊïàÊûúËá™ÁÑ∂‰∏î‰∏çÊîπÂèò‰∫∫Áâ©ÁöÑË∫´‰ªΩÁâπÂæÅ„ÄÇ"""

VTON_PROMPT_EN = """Use the green mask area in image 1 only to determine if the garment belongs to upper or lower body, do not restrict the garment to the mask area.

Naturally dress the person in image 1 with the garment from image 2, maintaining the complete shape, sleeve length, and silhouette of the garment from image 2. Whether image 2 shows the garment alone or on a model, accurately transfer the garment while preserving its original fabric texture, material details, and color accuracy.

Ensure the face, hair, and skin of the person in image 1 remain completely unchanged. Lighting and shadows should naturally match the environment of image 1, but the material appearance of the garment must stay faithful to image 2.

Keep edges smoothly blended, shadows realistic, and the overall effect natural without altering the person's identity features."""


def check_cuda():
    """Check CUDA availability and print GPU info."""
    print("=" * 60)
    print("üîç CUDA Environment Check")
    print("=" * 60)
    
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDA version: {torch.version.cuda}")
        print(f"Number of GPUs: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            total_memory = props.total_memory / (1024 ** 3)
            print(f"\nüìå GPU {i}: {props.name}")
            print(f"   Total Memory: {total_memory:.2f} GB")
            print(f"   Compute Capability: {props.major}.{props.minor}")
            
            # Check FP8 hardware support
            fp8_hw = props.major >= 9 or (props.major == 8 and props.minor >= 9)
            print(f"   FP8 Hardware: {'‚úÖ Native' if fp8_hw else '‚ö†Ô∏è Emulated'}")
    else:
        print("‚ùå CUDA is not available!")
        sys.exit(1)
    
    print("=" * 60)
    return True


def find_model_paths():
    """Find model paths in common locations."""
    possible_paths = [
        "models/Qwen-Image-Edit-2511",
        "./Qwen-Image-Edit-2511",
        "/models/Qwen-Image-Edit-2511",
        os.path.expanduser("~/.cache/huggingface/hub/models--Qwen--Qwen-Image-Edit-2511"),
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            return path
    
    return None


def find_lora_path():
    """Find LoRA weights path."""
    possible_paths = [
        "models/Qwen-Image-Edit-2511-Lightning/Qwen-Image-Edit-2511-Lightning-4steps-V1.0-fp32.safetensors",
        "models/Qwen-Image-Edit-2511-Lightning/Qwen-Image-Edit-2511-Lightning-4steps-V1.0-bf16.safetensors",
        "lora_weights/Qwen-Image-Edit-2511-Lightning-4steps-V1.0-bf16.safetensors",
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            return path
    
    return None


def find_fp8_path():
    """Find FP8 quantized weights path."""
    possible_paths = [
        "models/Qwen-Image-Edit-2511-Lightning/qwen_image_edit_2511_fp8_e4m3fn_scaled_lightning.safetensors",
        "models/qwen_image_edit_2511_fp8_e4m3fn_scaled_lightning.safetensors",
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            return path
    
    return None


def calculate_720p_resolution(person_image_path: str):
    """
    Calculate 720p output resolution maintaining input aspect ratio.
    720p = 720 pixels on the shorter side.
    """
    img = Image.open(person_image_path)
    width, height = img.size
    aspect_ratio = width / height
    
    # For 720p: shorter side = 720
    if width >= height:
        # Landscape or square: height = 720
        target_height = 720
        target_width = int(720 * aspect_ratio)
    else:
        # Portrait: width = 720
        target_width = 720
        target_height = int(720 / aspect_ratio)
    
    # Ensure dimensions are divisible by 16 (required for diffusion models)
    target_width = (target_width // 16) * 16
    target_height = (target_height // 16) * 16
    
    return target_width, target_height, aspect_ratio


def run_lightx2v_vton(
    person_image_path: str,
    cloth_image_path: str,
    output_path: str,
    mode: str = "lora",  # "lora", "fp8", or "base"
    enable_offload: bool = False,
    prompt: str = None,
    seed: int = 42,
    steps: int = 4,
    target_width: int = None,
    target_height: int = None,
    enable_teacache: bool = False,
    teacache_thresh: float = 0.26,
    warmup: bool = False,
):
    """
    Run Virtual Try-On using LightX2V framework.
    
    Args:
        person_image_path: Path to person image (with green mask)
        cloth_image_path: Path to cloth/garment image
        output_path: Path to save output
        mode: "lora" for BF16+LoRA, "fp8" for FP8+distillation, "base" for 40-step base
        enable_offload: Enable CPU offloading for low VRAM
        prompt: VTON prompt (uses Chinese prompt by default)
        seed: Random seed
        steps: Inference steps (4 for distilled, 40 for base)
        target_width: Target output width (auto-calculated if None)
        target_height: Target output height (auto-calculated if None)
        enable_teacache: Enable TeaCache for ~1.5-2x faster inference
        teacache_thresh: TeaCache threshold (0.26 default, lower = faster but less quality)
        warmup: Run warmup inference for consistent timing
    """
    from lightx2v import LightX2VPipeline
    
    # Calculate 720p resolution if not specified
    if target_width is None or target_height is None:
        target_width, target_height, aspect_ratio = calculate_720p_resolution(person_image_path)
        print(f"\nüìê Auto-calculated 720p resolution: {target_width}x{target_height} (AR: {aspect_ratio:.2f})")
    
    print("\n" + "=" * 60)
    print("üöÄ Initializing LightX2V Pipeline")
    print("=" * 60)
    print(f"Mode: {mode}")
    print(f"Offloading: {'‚úÖ Enabled' if enable_offload else '‚ùå Disabled'}")
    print(f"Output Resolution: {target_width}x{target_height}")
    print("-" * 60)
    
    # Find model path
    model_path = find_model_paths()
    if model_path is None:
        print("‚ùå Could not find Qwen-Image-Edit-2511 model!")
        print("   Run: huggingface-cli download Qwen/Qwen-Image-Edit-2511 --local-dir models/Qwen-Image-Edit-2511")
        sys.exit(1)
    
    print(f"Model path: {model_path}")
    
    # Clear GPU memory
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        import gc
        gc.collect()
        
        mem_before = torch.cuda.memory_allocated() / (1024 ** 3)
        print(f"üìä GPU Memory before loading: {mem_before:.2f} GB")
    
    start_time = time.time()
    
    # Initialize LightX2V pipeline
    pipe = LightX2VPipeline(
        model_path=model_path,
        model_cls="qwen-image-edit-2511",
        task="i2i",
    )
    
    # Enable offloading if requested (for low VRAM)
    if enable_offload:
        print("\nüîß Enabling CPU offloading...")
        pipe.enable_offload(
            cpu_offload=True,
            offload_granularity="block",
            text_encoder_offload=True,
            vae_offload=False,
        )
        print("‚úÖ Offloading enabled")
    
    # Configure based on mode
    if mode == "fp8":
        fp8_path = find_fp8_path()
        if fp8_path is None:
            print("‚ö†Ô∏è FP8 weights not found, falling back to LoRA mode")
            mode = "lora"
        else:
            print(f"\nüîß Enabling FP8 quantization...")
            print(f"   FP8 weights: {fp8_path}")
            pipe.enable_quantize(
                dit_quantized=True,
                dit_quantized_ckpt=fp8_path,
                quant_scheme="fp8-sgl"
            )
            steps = 4  # FP8 model has distillation baked in
            print("‚úÖ FP8 quantization enabled")
    
    if mode == "lora":
        lora_path = find_lora_path()
        if lora_path is None:
            print("‚ö†Ô∏è LoRA weights not found!")
            print("   Run: huggingface-cli download lightx2v/Qwen-Image-Edit-2511-Lightning --local-dir models/Qwen-Image-Edit-2511-Lightning")
            sys.exit(1)
        
        print(f"\nüîß Loading 4-step Lightning LoRA...")
        print(f"   LoRA path: {lora_path}")
        pipe.enable_lora([
            {"path": lora_path, "strength": 1.0},
        ])
        steps = 4
        print("‚úÖ LoRA loaded")
    
    if mode == "base":
        steps = 40
        print("\n‚ö†Ô∏è Using base model (40 steps, slower)")
    
    # Determine attention mode
    # L40/L40S: Use flash_attn2 (FA3 is Hopper-only)
    # LightX2V supported modes: "flash_attn2", "flash_attn3", "torch_sdpa", "sage_attn2"
    attn_mode = "torch_sdpa"  # Safe default - uses PyTorch SDPA
    
    # Detect GPU and choose optimal attention backend
    gpu_name = torch.cuda.get_device_name(0).lower() if torch.cuda.is_available() else ""
    
    # Check if flash_attn is available
    try:
        import importlib.util
        if importlib.util.find_spec("flash_attn") is not None:
            import flash_attn
            # L40/L40S or other Ada GPUs: Use FA2 (FA3 is Hopper-only)
            if "l40" in gpu_name or "4090" in gpu_name or "4080" in gpu_name:
                attn_mode = "flash_attn2"
                print(f"\nüîß L40/Ada GPU detected ‚Üí Using Flash Attention 2")
            elif "h100" in gpu_name or "h200" in gpu_name:
                # Only Hopper GPUs can use FA3
                try:
                    from flash_attn_interface import flash_attn_func
                    attn_mode = "flash_attn3"
                    print(f"\nüîß Hopper GPU detected ‚Üí Using Flash Attention 3")
                except ImportError:
                    attn_mode = "flash_attn2"
                    print(f"\nüîß Hopper GPU but FA3 not installed ‚Üí Using Flash Attention 2")
            else:
                attn_mode = "flash_attn2"
                print(f"\nüîß Using Flash Attention 2")
    except Exception as e:
        print(f"\nüîß Flash Attention unavailable ({type(e).__name__}), using PyTorch SDPA")
        attn_mode = "torch_sdpa"
    
    # Create generator with resolution settings
    print(f"\nüîß Creating generator (steps={steps})...")
    
    # IMPORTANT: Detect input image aspect ratio BEFORE creating generator
    # to prevent zooming/cropping
    person_img_check = Image.open(person_image_path)
    orig_w, orig_h = person_img_check.size
    orig_ratio = orig_w / orig_h
    person_img_check.close()
    
    # Adjust target dimensions to match input aspect ratio
    if orig_ratio < 1:  # Portrait (taller than wide)
        # Keep height at target, adjust width
        actual_height = target_height
        actual_width = int(target_height * orig_ratio)
    else:  # Landscape or square
        # Keep width at target, adjust height
        actual_width = target_width
        actual_height = int(target_width / orig_ratio)
    
    # Ensure dimensions are multiples of 16
    actual_width = max(16, (actual_width // 16) * 16)
    actual_height = max(16, (actual_height // 16) * 16)
    
    print(f"üìê Input: {orig_w}x{orig_h} ‚Üí Output: {actual_width}x{actual_height} (preserving aspect ratio)")
    
    # Determine aspect_ratio for LightX2V resolution control
    # LightX2V default mappings:
    #   "16:9": [1664, 928], "9:16": [928, 1664], "1:1": [1328, 1328]
    #   "4:3": [1472, 1140], "3:4": [768, 1024]
    if orig_ratio < 0.8:  # Portrait (taller than wide)
        target_aspect_ratio = "3:4"  # Maps to 768x1024
        print(f"   Setting aspect_ratio: 3:4 (portrait) ‚Üí 768x1024")
    elif orig_ratio > 1.2:  # Landscape (wider than tall)  
        target_aspect_ratio = "4:3"  # Maps to 1472x1140
        print(f"   Setting aspect_ratio: 4:3 (landscape) ‚Üí 1472x1140")
    else:  # Square-ish
        target_aspect_ratio = "1:1"  # Maps to 1328x1328
        print(f"   Setting aspect_ratio: 1:1 (square) ‚Üí 1328x1328")
    
    # Pass aspect_ratio directly to create_generator (this is the proper LightX2V API)
    pipe.create_generator(
        attn_mode=attn_mode,
        infer_steps=steps,
        guidance_scale=1.0,
        width=actual_width,
        height=actual_height,
        aspect_ratio=target_aspect_ratio,  # This sets config aspect_ratio properly
    )
    
    # Enable TeaCache AFTER creating generator (model must be loaded first)
    # We monkey-patch the transformer_infer with our TeaCache implementation
    teacache_enabled = False
    if enable_teacache:
        print(f"\n‚ö° Enabling TeaCache (threshold={teacache_thresh})...")
        # Import our TeaCache implementation
        import sys
        script_dir = os.path.dirname(os.path.abspath(__file__))
        if script_dir not in sys.path:
            sys.path.insert(0, script_dir)
        try:
            from teacache_transformer_infer import QwenImageTeaCacheTransformerInfer
            
            # Get the current transformer_infer
            orig_transformer_infer = pipe.runner.model.transformer_infer
            
            # Create new TeaCache infer instance with same config
            config = orig_transformer_infer.config.copy()
            config["teacache_thresh"] = teacache_thresh
            config["coefficients"] = [1.0]  # Linear scaling
            config["infer_steps"] = steps
            
            # Create TeaCache wrapper
            teacache_infer = QwenImageTeaCacheTransformerInfer(config)
            teacache_infer.scheduler = orig_transformer_infer.scheduler
            teacache_infer.infer_func = orig_transformer_infer.infer_func
            
            # Replace the transformer_infer
            pipe.runner.model.transformer_infer = teacache_infer
            
            teacache_enabled = True
            print("‚úÖ TeaCache enabled")
            print(f"   Threshold: {teacache_thresh}")
            print(f"   Expected: Skip 1-2 transformer passes ‚Üí 25-50% speedup")
        except Exception as e:
            print(f"‚ö†Ô∏è TeaCache setup failed: {e}")
            import traceback
            traceback.print_exc()
            print("   Continuing without TeaCache...")
    
    init_time = time.time() - start_time
    print(f"‚úÖ Pipeline initialized in {init_time:.2f} seconds")
    
    # Memory usage
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / (1024 ** 3)
        total = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
        print(f"üìä GPU Memory: {allocated:.2f} GB / {total:.2f} GB")
    
    print("=" * 60)
    
    # Pre-resize images to calculated resolution (actual_width x actual_height)
    import tempfile
    temp_dir = tempfile.mkdtemp()
    
    # Resize person image to aspect-ratio-correct dimensions
    person_img = Image.open(person_image_path)
    person_resized = person_img.resize((actual_width, actual_height), Image.LANCZOS)
    person_temp = os.path.join(temp_dir, "person_resized.png")
    person_resized.save(person_temp)
    print(f"üìê Person resized to: {actual_width}x{actual_height}")
    
    # Resize cloth image proportionally to match person
    cloth_img = Image.open(cloth_image_path)
    cloth_ratio = cloth_img.width / cloth_img.height
    person_ratio = actual_width / actual_height
    if cloth_ratio > person_ratio:
        cloth_w = actual_width
        cloth_h = int(actual_width / cloth_ratio)
    else:
        cloth_h = actual_height
        cloth_w = int(actual_height * cloth_ratio)
    # Ensure dimensions are at least 16x16
    cloth_w = max(16, (cloth_w // 16) * 16)
    cloth_h = max(16, (cloth_h // 16) * 16)
    cloth_resized = cloth_img.resize((cloth_w, cloth_h), Image.LANCZOS)
    cloth_temp = os.path.join(temp_dir, "cloth_resized.png")
    cloth_resized.save(cloth_temp)
    
    print(f"üìê Cloth: {cloth_img.width}x{cloth_img.height} ‚Üí {cloth_w}x{cloth_h}")
    
    # Use resized images
    image_paths = f"{person_temp},{cloth_temp}"
    
    # Use prompt
    if prompt is None:
        prompt = VTON_PROMPT_CN
    
    print("\n" + "=" * 60)
    print("üëó Running Virtual Try-On Inference")
    print("=" * 60)
    print(f"Person image: {person_image_path} -> {target_width}x{target_height}")
    print(f"Cloth image: {cloth_image_path} -> {cloth_w}x{cloth_h}")
    print(f"Steps: {steps}")
    print(f"Mode: {mode}")
    print(f"Seed: {seed}")
    print("-" * 60)
    
    # Reset peak memory tracking
    if torch.cuda.is_available():
        torch.cuda.reset_peak_memory_stats()
    
    print("\nüîÑ Starting inference...")
    infer_start = time.time()
    
    # Generate - resolution is controlled by input image size
    pipe.generate(
        seed=seed,
        image_path=image_paths,
        prompt=prompt,
        negative_prompt="",
        save_result_path=output_path,
    )
    
    # Cleanup temp files
    import shutil
    shutil.rmtree(temp_dir, ignore_errors=True)
    
    inference_time = time.time() - infer_start
    
    print(f"\n‚úÖ Inference completed!")
    print(f"‚è±Ô∏è  Time: {inference_time:.2f} seconds")
    print(f"‚ö° Speed: {inference_time / steps:.2f} seconds/step")
    print(f"üíæ Output saved to: {os.path.abspath(output_path)}")
    
    # Memory stats
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / (1024 ** 3)
        peak = torch.cuda.max_memory_allocated() / (1024 ** 3)
        print(f"üìä GPU Memory - Current: {allocated:.2f} GB, Peak: {peak:.2f} GB")
    
    print("=" * 60)
    
    return output_path, inference_time


def create_comparison_image(
    person_path: str,
    cloth_path: str,
    result_path: str,
    output_path: str
):
    """Create side-by-side comparison."""
    person_img = Image.open(person_path).convert("RGB")
    cloth_img = Image.open(cloth_path).convert("RGB")
    result_img = Image.open(result_path).convert("RGB")
    
    target_height = 768
    
    def resize_to_height(img, height):
        ratio = height / img.height
        new_width = int(img.width * ratio)
        return img.resize((new_width, height), Image.LANCZOS)
    
    person_resized = resize_to_height(person_img, target_height)
    cloth_resized = resize_to_height(cloth_img, target_height)
    result_resized = resize_to_height(result_img, target_height)
    
    total_width = person_resized.width + cloth_resized.width + result_resized.width + 40
    comparison = Image.new('RGB', (total_width, target_height + 60), color=(40, 40, 40))
    
    x_offset = 10
    comparison.paste(person_resized, (x_offset, 50))
    x_offset += person_resized.width + 10
    comparison.paste(cloth_resized, (x_offset, 50))
    x_offset += cloth_resized.width + 10
    comparison.paste(result_resized, (x_offset, 50))
    
    try:
        from PIL import ImageDraw, ImageFont
        draw = ImageDraw.Draw(comparison)
        
        try:
            font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 20)
        except:
            font = ImageFont.load_default()
        
        x_offset = 10
        draw.text((x_offset + person_resized.width//2 - 30, 15), "Person", fill=(255, 255, 255), font=font)
        x_offset += person_resized.width + 10
        draw.text((x_offset + cloth_resized.width//2 - 25, 15), "Cloth", fill=(255, 255, 255), font=font)
        x_offset += cloth_resized.width + 10
        draw.text((x_offset + result_resized.width//2 - 30, 15), "Result", fill=(255, 255, 255), font=font)
    except Exception as e:
        print(f"Note: Could not add labels: {e}")
    
    comparison.save(output_path)
    print(f"üì∏ Comparison image saved to: {output_path}")
    return comparison


def main():
    parser = argparse.ArgumentParser(
        description="LightX2V Virtual Try-On with Qwen-Image-Edit-2511",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Run with BF16 + 4-step Lightning LoRA
  python test_lightx2v_vton.py --mode lora

  # Run with FP8 + distillation (fastest, lowest memory)
  python test_lightx2v_vton.py --mode fp8

  # Run with CPU offloading (for low VRAM GPUs)
  python test_lightx2v_vton.py --mode fp8 --offload

  # Custom images
  python test_lightx2v_vton.py --person my_photo.jpg --cloth my_garment.png --mode fp8
        """
    )
    parser.add_argument("--person", "-p", type=str, default="person.jpg",
                        help="Path to person image (with green mask)")
    parser.add_argument("--cloth", "-c", type=str, default="cloth.png",
                        help="Path to cloth/garment image")
    parser.add_argument("--output", "-o", type=str, default="outputs/vton_lightx2v_result.png",
                        help="Output path for result")
    parser.add_argument("--mode", "-m", type=str, default="lora",
                        choices=["lora", "fp8", "base"],
                        help="Inference mode: lora (BF16+LoRA), fp8 (FP8+distill), base (40 steps)")
    parser.add_argument("--offload", action="store_true",
                        help="Enable CPU offloading for low VRAM")
    parser.add_argument("--seed", type=int, default=42,
                        help="Random seed")
    parser.add_argument("--prompt-lang", type=str, default="cn", choices=["cn", "en"],
                        help="Prompt language")
    parser.add_argument("--no-comparison", action="store_true",
                        help="Skip creating comparison image")
    parser.add_argument("--width", type=int, default=None,
                        help="Target output width (default: auto-calculate for 720p)")
    parser.add_argument("--height", type=int, default=None,
                        help="Target output height (default: auto-calculate for 720p)")
    parser.add_argument("--resolution", type=str, default="720p",
                        choices=["480p", "720p", "1080p", "auto"],
                        help="Target resolution preset (default: 720p)")
    parser.add_argument("--teacache", action="store_true",
                        help="Enable TeaCache for faster inference (quality trade-off)")
    parser.add_argument("--teacache-thresh", type=float, default=0.08,
                        help="TeaCache threshold (lower = better quality, default: 0.08)")
    parser.add_argument("--warmup", action="store_true",
                        help="Run warmup inference before timing")
    
    args = parser.parse_args()
    
    print("\n" + "‚ö°" * 30)
    print("   LIGHTX2V VIRTUAL TRY-ON TEST")
    print("   Qwen-Image-Edit-2511 ‚ö°")
    print("‚ö°" * 30 + "\n")
    
    # Check CUDA
    check_cuda()
    
    # Validate inputs
    if not os.path.exists(args.person):
        print(f"‚ùå Person image not found: {args.person}")
        sys.exit(1)
    
    if not os.path.exists(args.cloth):
        print(f"‚ùå Cloth image not found: {args.cloth}")
        sys.exit(1)
    
    # Create output directory
    os.makedirs(os.path.dirname(args.output) or ".", exist_ok=True)
    
    # Select prompt
    prompt = VTON_PROMPT_CN if args.prompt_lang == "cn" else VTON_PROMPT_EN
    
    # Calculate resolution
    target_width = args.width
    target_height = args.height
    
    if target_width is None or target_height is None:
        # Auto-calculate based on resolution preset
        img = Image.open(args.person)
        w, h = img.size
        aspect_ratio = w / h
        
        # Resolution presets (shorter side)
        res_map = {"480p": 480, "720p": 720, "1080p": 1080, "auto": None}
        target_short = res_map.get(args.resolution)
        
        if target_short and args.resolution != "auto":
            if w >= h:
                target_height = target_short
                target_width = int(target_short * aspect_ratio)
            else:
                target_width = target_short
                target_height = int(target_short / aspect_ratio)
            
            # Ensure divisible by 16
            target_width = (target_width // 16) * 16
            target_height = (target_height // 16) * 16
        
        print(f"üìê Resolution: {args.resolution} -> {target_width}x{target_height}")
    
    # Run inference
    result_path, inference_time = run_lightx2v_vton(
        person_image_path=args.person,
        cloth_image_path=args.cloth,
        output_path=args.output,
        mode=args.mode,
        enable_offload=args.offload,
        prompt=prompt,
        seed=args.seed,
        target_width=target_width,
        target_height=target_height,
        enable_teacache=args.teacache,
        teacache_thresh=args.teacache_thresh,
        warmup=args.warmup,
    )
    
    # Create comparison
    if not args.no_comparison:
        comparison_path = args.output.replace(".png", "_comparison.png")
        create_comparison_image(args.person, args.cloth, result_path, comparison_path)
    
    # Summary
    print("\n" + "=" * 60)
    print("üìã TEST SUMMARY")
    print("=" * 60)
    print(f"Framework: LightX2V")
    print(f"Model: Qwen-Image-Edit-2511")
    print(f"Mode: {args.mode}")
    print(f"Offload: {'Yes' if args.offload else 'No'}")
    print(f"Inference Time: {inference_time:.2f}s")
    print(f"Output: {os.path.abspath(result_path)}")
    print("=" * 60)
    
    print("\n" + "‚úÖ" * 30)
    print("   VIRTUAL TRY-ON COMPLETED!")
    print("‚úÖ" * 30 + "\n")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
