#!/usr/bin/env python3
"""
Qwen-Image-Edit-2511 SDNQ UINT4 Test Script

Uses SDNQ (Squeezed Data Neural Quantization) for 4-bit quantization.
This is DIRECTLY compatible with diffusers!

Model: Disty0/Qwen-Image-Edit-2511-SDNQ-uint4-svd-r32
- 4-bit quantization with SVD rank 32
- Expected VRAM: ~12-15GB
- Compatible with QwenImageEditPlusPipeline

Requirements:
- pip install sdnq
"""

import argparse
import math
import os
import sys
import time

import torch
from PIL import Image


def check_cuda():
    """Check CUDA availability and print GPU info."""
    print("=" * 60)
    print("ğŸ” CUDA Environment Check")
    print("=" * 60)
    
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDA version: {torch.version.cuda}")
        print(f"Number of GPUs: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            total_memory = props.total_memory / (1024 ** 3)
            print(f"\nğŸ“Œ GPU {i}: {props.name}")
            print(f"   Total Memory: {total_memory:.2f} GB")
    else:
        print("âŒ CUDA is not available!")
        sys.exit(1)
    
    print("=" * 60)
    return True


def load_pipeline_sdnq(device: str = "cuda"):
    """Load the SDNQ 4-bit quantized pipeline."""
    import diffusers
    from diffusers import FlowMatchEulerDiscreteScheduler
    
    # Import SDNQ - this registers it into diffusers
    from sdnq import SDNQConfig
    from sdnq.common import use_torch_compile as triton_is_available
    from sdnq.loader import apply_sdnq_options_to_model
    
    model_id = "Disty0/Qwen-Image-Edit-2511-SDNQ-uint4-svd-r32"
    
    print("\n" + "=" * 60)
    print("ğŸš€ Loading Qwen-Image-Edit-2511 SDNQ UINT4 Pipeline")
    print("=" * 60)
    print(f"Model: {model_id}")
    print(f"Quantization: 4-bit UINT4 with SVD rank 32")
    print(f"Expected VRAM: ~12-15GB")
    print("-" * 60)
    
    # Clear GPU memory
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        import gc
        gc.collect()
    
    start_time = time.time()
    
    # Scheduler config for distilled model (4 steps)
    scheduler_config = {
        "base_image_seq_len": 256,
        "base_shift": math.log(3),
        "invert_sigmas": False,
        "max_image_seq_len": 8192,
        "max_shift": math.log(3),
        "num_train_timesteps": 1000,
        "shift": 1.0,
        "shift_terminal": None,
        "stochastic_sampling": False,
        "time_shift_type": "exponential",
        "use_beta_sigmas": False,
        "use_dynamic_shifting": True,
        "use_exponential_sigmas": False,
        "use_karras_sigmas": False,
    }
    scheduler = FlowMatchEulerDiscreteScheduler.from_config(scheduler_config)
    
    # Load the SDNQ quantized pipeline
    print("Loading SDNQ quantized pipeline...")
    pipeline = diffusers.QwenImageEditPlusPipeline.from_pretrained(
        model_id,
        scheduler=scheduler,
        torch_dtype=torch.bfloat16,
    )
    
    # Enable INT8 MatMul for faster inference on NVIDIA GPUs
    if triton_is_available and torch.cuda.is_available():
        print("Enabling INT8 MatMul optimization...")
        pipeline.transformer = apply_sdnq_options_to_model(
            pipeline.transformer, use_quantized_matmul=True
        )
        pipeline.text_encoder = apply_sdnq_options_to_model(
            pipeline.text_encoder, use_quantized_matmul=True
        )
        print("âœ… INT8 MatMul enabled!")
    
    # Enable CPU offload for memory efficiency
    print("Enabling model CPU offload...")
    pipeline.enable_model_cpu_offload()
    
    print(f"âœ… Pipeline loaded in {time.time() - start_time:.2f} seconds")
    
    # Print memory usage
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / (1024 ** 3)
        total = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
        print(f"ğŸ“Š GPU Memory: {allocated:.2f} GB / {total:.2f} GB")
    
    pipeline.set_progress_bar_config(disable=False)
    
    print("=" * 60)
    return pipeline


def run_inference(pipeline, 
                  images: list,
                  prompt: str,
                  output_path: str = "output_image.png",
                  num_inference_steps: int = 4,
                  true_cfg_scale: float = 1.0,
                  seed: int = 42):
    """Run inference with the pipeline."""
    print("\n" + "=" * 60)
    print("ğŸ¨ Running Virtual Try-On Inference (SDNQ UINT4)")
    print("=" * 60)
    print(f"Number of input images: {len(images)}")
    print(f"Inference steps: {num_inference_steps}")
    print(f"True CFG scale: {true_cfg_scale}")
    print(f"Seed: {seed}")
    print("-" * 60)
    print(f"Prompt:\n{prompt[:200]}..." if len(prompt) > 200 else f"Prompt:\n{prompt}")
    print("-" * 60)
    
    inputs = {
        "image": images,
        "prompt": prompt,
        "generator": torch.manual_seed(seed),
        "true_cfg_scale": true_cfg_scale,
        "negative_prompt": " ",
        "num_inference_steps": num_inference_steps,
        "guidance_scale": 1.0,
    }
    
    start_time = time.time()
    
    with torch.inference_mode():
        output = pipeline(**inputs)
    
    inference_time = time.time() - start_time
    print(f"\nâœ… Inference completed in {inference_time:.2f} seconds")
    print(f"âš¡ Speed: {inference_time / num_inference_steps:.2f} seconds per step")
    
    # Save output
    output_image = output.images[0]
    output_image.save(output_path)
    print(f"ğŸ’¾ Output saved to: {os.path.abspath(output_path)}")
    
    # Print final memory usage
    if torch.cuda.is_available():
        max_allocated = torch.cuda.max_memory_allocated() / (1024 ** 3)
        print(f"ğŸ“Š Peak GPU Memory: {max_allocated:.2f} GB")
    
    print("=" * 60)
    return output_image


def main():
    parser = argparse.ArgumentParser(
        description="Qwen-Image-Edit-2511 SDNQ UINT4 Test",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
This uses the SDNQ 4-bit quantized model which requires only ~12-15GB VRAM!

Install requirements:
    pip install sdnq

Examples:
    python test_qwen_sdnq.py --person person.jpg --cloth cloth.png
    python test_qwen_sdnq.py --person person.jpg --cloth cloth.png --steps 8
        """
    )
    parser.add_argument("--person", "-p", type=str, required=True,
                        help="Path to person image (with green mask)")
    parser.add_argument("--cloth", "-c", type=str, required=True,
                        help="Path to cloth/garment image")
    parser.add_argument("--output", "-o", type=str, default="vton_sdnq_output.png",
                        help="Output path")
    parser.add_argument("--prompt", type=str, default=None,
                        help="Custom prompt (uses VTON prompt by default)")
    parser.add_argument("--steps", type=int, default=4,
                        help="Inference steps (default: 4)")
    parser.add_argument("--cfg", type=float, default=1.0,
                        help="True CFG scale (default: 1.0 for distilled model)")
    parser.add_argument("--seed", type=int, default=42,
                        help="Random seed")
    
    args = parser.parse_args()
    
    # Default VTON prompt
    if args.prompt is None:
        args.prompt = """å°†å›¾ç‰‡ 1 ä¸­çš„ç»¿è‰²é®ç½©åŒºåŸŸä»…ç”¨äºåˆ¤æ–­æœè£…å±äºä¸ŠåŠèº«æˆ–ä¸‹åŠèº«ï¼Œä¸è¦å°†æœè£…é™åˆ¶åœ¨é®ç½©èŒƒå›´å†…ã€‚

å°†å›¾ç‰‡ 2 ä¸­çš„æœè£…è‡ªç„¶åœ°ç©¿æˆ´åˆ°å›¾ç‰‡ 1 ä¸­çš„äººç‰©èº«ä¸Šï¼Œä¿æŒå›¾ç‰‡ 2 ä¸­æœè£…çš„å®Œæ•´å½¢çŠ¶ã€è¢–é•¿å’Œè½®å»“ã€‚æ— è®ºå›¾ç‰‡ 2 æ˜¯å•ç‹¬çš„æœè£…å›¾è¿˜æ˜¯äººç‰©ç©¿ç€è¯¥æœè£…çš„å›¾ï¼Œéƒ½åº”å‡†ç¡®åœ°è½¬ç§»æœè£…ï¼ŒåŒæ—¶ä¿ç•™å…¶åŸå§‹é¢æ–™è´¨æ„Ÿã€æè´¨ç»†èŠ‚å’Œé¢œè‰²å‡†ç¡®æ€§ã€‚

ç¡®ä¿å›¾ç‰‡ 1 ä¸­äººç‰©çš„é¢éƒ¨ã€å¤´å‘å’Œçš®è‚¤å®Œå…¨ä¿æŒä¸å˜ã€‚å…‰ç…§ä¸é˜´å½±åº”è‡ªç„¶åŒ¹é…å›¾ç‰‡ 1 çš„ç¯å¢ƒï¼Œä½†æœè£…çš„æè´¨å¤–è§‚å¿…é¡»å¿ å®äºå›¾ç‰‡ 2ã€‚

ä¿æŒè¾¹ç¼˜å¹³æ»‘èåˆã€é˜´å½±é€¼çœŸï¼Œæ•´ä½“æ•ˆæœè‡ªç„¶ä¸”ä¸æ”¹å˜äººç‰©çš„èº«ä»½ç‰¹å¾ã€‚"""
    
    print("\n" + "ğŸ‘—" * 30)
    print("   QWEN VIRTUAL TRY-ON (SDNQ UINT4)")
    print("   4-bit Quantized - Low VRAM Mode!")
    print("ğŸ‘—" * 30 + "\n")
    
    # Check CUDA
    check_cuda()
    
    # Check input files
    if not os.path.exists(args.person):
        print(f"âŒ Person image not found: {args.person}")
        sys.exit(1)
    if not os.path.exists(args.cloth):
        print(f"âŒ Cloth image not found: {args.cloth}")
        sys.exit(1)
    
    # Load images
    print("\nğŸ“· Loading input images...")
    person_img = Image.open(args.person).convert("RGB")
    cloth_img = Image.open(args.cloth).convert("RGB")
    print(f"   Person image: {person_img.size}")
    print(f"   Cloth image: {cloth_img.size}")
    
    # Load pipeline
    pipeline = load_pipeline_sdnq()
    
    # Run inference with both images
    output_image = run_inference(
        pipeline=pipeline,
        images=[person_img, cloth_img],
        prompt=args.prompt,
        output_path=args.output,
        num_inference_steps=args.steps,
        true_cfg_scale=args.cfg,
        seed=args.seed,
    )
    
    print("\n" + "âœ…" * 30)
    print("   VIRTUAL TRY-ON COMPLETED!")
    print("   Using SDNQ 4-bit quantization")
    print("âœ…" * 30 + "\n")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
